---
output: 
  html_document: 
    keep_md: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```
#Data Science Using R
##3 - Getting and Cleaning Data

###0) R version used
```{r}
print(R.version.string)
```

###1) downloading/reading-local/reading-xls/reading-XML/XPath/JSON/data.table
```{r}
# downloading
if(!file.exists("data")){ dir.create("data") }

# reading local
if(!file.exists("data/cameras.csv")){
    fileUrl<-"https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
    download.file(fileUrl,destfile="./data/cameras.csv",method="curl")
}
cameraData<-read.table("./data/cameras.csv",sep=",",header=TRUE)
head(cameraData,2)
str(cameraData)

# reading xls
if(!file.exists("data/cameras.xlsx")){
    fileUrl<-"https://data.baltimorecity.gov/api/views/dz54-2aru/rows.xlsx?accessType=DOWNLOAD"
    download.file(fileUrl,destfile="./data/cameras.xlsx",method="curl")
}

if(!require(xlsx)){
  install.packages("xlsx",repo="https://cran.rstudio.com/")
}
library(xlsx)
cameraData<-read.xlsx("./data/cameras.xlsx",sheetIndex=1,header=TRUE)
head(cameraData,2)
str(cameraData)
cameraDataSubset<-read.xlsx("./data/cameras.xlsx",sheetIndex=1,colIndex=2:3,rowIndex=1:4)
head(cameraDataSubset,2)
str(cameraDataSubset)

# reading XML
if(!file.exists("data/simple.xml")){
    fileUrl<-"http://www.w3schools.com/xml/simple.xml"
    download.file(fileUrl,destfile="./data/simple.xml",method="curl")
}
if(!require(XML)){
  install.packages("XML",repo="https://cran.rstudio.com/")
}
library(XML)
doc<-xmlTreeParse("./data/simple.xml",useInternal=TRUE)
rootNode<-xmlRoot(doc)
names(rootNode[[1]])
rootNode[[1]]
rootNode[[1]][[2]]

# programmatically extracting parts of file
xmlSApply(rootNode,xmlValue)
# get the item menu/price using XPath
xpathSApply(rootNode,"//name",xmlValue)
xpathSApply(rootNode,"//price",xmlValue)

# extract content by attributes using XPath
if(!file.exists("data/baltimore-ravens.html")){
    fileUrl<-"http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
    download.file(fileUrl,destfile="./data/baltimore-ravens.html",method="curl")
}
doc<-htmlTreeParse("./data/baltimore-ravens.html",useInternal=TRUE)
xpathSApply(doc,"//li[@class='team-name']",xmlValue)

# reading JSON
if(!file.exists("data/jtleek.json")){
    fileUrl<-"https://api.github.com/users/jtleek/repos"
    download.file(fileUrl,destfile="./data/jtleek.json",method="curl")
}
if(!require(jsonlite)){
  install.packages("jsonlite",repo="https://cran.rstudio.com/")
}
library(jsonlite)
jsonData<-fromJSON("./data/jtleek.json")
names(jsonData)

#nested objects JSON
library(datasets)
names(jsonData$owner)
testjson<-toJSON(data.frame(foo=1:4,bar=c(T,T,F,F)),pretty=TRUE)
testjson
head(fromJSON(testjson))

# data.table (written in C / VERY FAST)
# inherits from data.frame
if(!require(data.table)){
  install.packages("data.table",repo="https://cran.rstudio.com/")
}
library(data.table)
set.seed(37)
df=data.frame(x=rbinom(5,2,0.5),y=rep(c("a","b","c","d","e")))
dt=data.table(x=rbinom(5,2,0.5),y=rep(c("a","b","c","d","e")))
dt
# subset of data.table
dt[2,] # dt where row is 2 and all of column
dt[dt$y=="a",] # dt where row is where dt's y value equals to "a" and all of col
dt[c(2,3)] # dt where row is 2 and 3 and all of col
dt[,c(2,3)] # subsetting col doesn't work the same as for dataframe

dt[,list(mean(x),sum(x))] # obtains a list of mean of x values of dt, and sum of x
dt[,table(y)] # ~table(dt$y); obtains a table of y values in dt
dt[,z:=x^2] # creates a new col z that is x ^ 2 
dt2<-dt # this does not actually create a new/separate copy
dt2[,y:=2]
head(dt2)
head(dt) # dt2<-dt does not actually create a new copy

# multiple operations within creating a new col
dt[,m:={tmp<-(x+z);log2(tmp+5)}]
head(dt)

# plyr like operations
dt[,a:=x>0]
head(dt)

# by a means where a condition is TRUE, it takes the mean of x and z
# and when a is false, collect those values and perform the func
dt[,b:=sum(x+z),by=a]
head(dt)

# special variables: .N
set.seed(123)
dt<-data.table(x=sample(letters[1:2],5,TRUE))
head(dt)
dt[,.N,by=x] # .N counts by x

# special variables: Keys
dt<-data.table(x=rep(c("a","b","c"),each=3),y=rnorm(3))
setkey(dt,x) # set x as the key so that it can be extracted as below
dt
dt["a"]

dt1<-data.table(x=c("a","a","b","dt1"),y=1:4)
dt2<-data.table(x=c("a","b","dt2"),z=5:7)
setkey(dt1,x); setkey(dt2,x)
dt1
dt2
# hard to interpret, but when x = a, y = 1 and 2 (dt1) and z = 5 (dt2), hence
# x=a;y=1;z=5
# x=a;y=2;z=5
# when x = b, y = 3 (dt1) and z = 6 hence
# x = b;y=3;z=6
# since there is no common x key value for dt1 and dt2 for dt1 and dt2, it is omitted
merge(dt1,dt2) 

# fast reading
big_df<-data.frame(x=rnorm(1E6),y=rnorm(1E6))
file<-tempfile()
write.table(big_df,file=file,row.names=FALSE,col.names=TRUE,sep="\t",quote=FALSE)
if(!require(microbenchmark)){
  install.packages("microbenchmark",repo="https://cran.rstudio.com/")
}
library(microbenchmark)
#fast reading
microbenchmark(fread(file),times=1)
# normal read table
microbenchmark(read.table(file,header=TRUE,sep="\t"),times=1)
```

###2) reading MySQL
```{r}
if(!require(RMySQL)){
  install.packages("RMySQL",repo="https://cran.rstudio.com/")
}
library(RMySQL)
# connect and obtain result
ucscDb<-dbConnect(MySQL(),user="genome",host="genome-mysql.cse.ucsc.edu")
result<-dbGetQuery(ucscDb,"show databases;"); dbDisconnect(ucscDb);

# connecting to hg19 db and listing all the tables
hg19<-dbConnect(MySQL(),user="genome",db="hg19",host="genome-mysql.cse.ucsc.edu")
allTables<-dbListTables(hg19)
length(allTables)
allTables[1:5]

# get dimensions of specific table
dbListFields(hg19,"affyU133Plus2")
dbGetQuery(hg19,"select count(*) from affyU133Plus2")

# read from a table
affyData<-dbReadTable(hg19,"affyU133Plus2")
str(affyData)

# select a specific subset
query<-dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
affyMis<-fetch(query); quantile(affyMis$misMatches)
affyMisSmall<-fetch(query,n=10); dbClearResult(query);
dim(affyMisSmall)
dbDisconnect(hg19)
```

###3) reading HDF5
```{r}
# heirarchical data format
# groups contain zero or more data sets/metadata
#       group header with grou name and list of attributes
#       group symbol table with list of objects in group
# datasets multidimensional array of data elements with metadata
#       have header with name,datatype,dataspace,storage layout
#       have data array with data
#  http:/www.hdfgroup.org

if(!file.exists("/data/biocLite.R")){
    fileUrl<-"http://bioconductor.org/biocLite.R"
    download.file(fileUrl,"./data/biocLite.R",method="curl")
}
source("http://bioconductor.org/biocLite.R")
if(!require(rhdf5)){
  biocLite("rhdf5")
}
library(rhdf5)
if(file.exists("example.h5")){
    file.remove("example.h5")
}

created<-h5createFile("example.h5")
created

# create groups
created<-h5createGroup("example.h5","foo")
created<-h5createGroup("example.h5","baa")
created<-h5createGroup("example.h5","foo/foobaa")
h5ls("example.h5")

# write to groups
A<-matrix(1:10,nr=5,nc=2)
h5write(A,"example.h5","foo/A")
B<-array(seq(0.1,2.0,by=0.1),dim=c(5,2,2))
attr(B,"scale")<-"liter"
h5write(B,"example.h5","foo/foobaa/B")
h5ls("example.h5")

# reading data
readA<-h5read("example.h5","foo/A")
readA
readA<-h5read("example.h5","foo/foobaa/B")
readA

# writing and reading chunks
h5write(c(12,13,14),"example.h5","foo/A",index=list(1:3,1)) # write 12,13,14 into rows 1 through 3 column 1 of foo/A within example.h5
h5read("example.h5","foo/A")
```

###4) reading from the web
```{r}
# just in case google blocks the ip for accessing too often
if(!file.exists("data/google-scholar.html")){
    fileUrl<-"http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
    download.file(fileUrl,"./data/google-scholar.html",method="curl")
}
htmlurl<-"http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
con = url(htmlurl)
htmlCode<-readLines(con)
close(con)

# parsing with XML
library(XML)
html<-htmlTreeParse(htmlurl,useInternalNodes=TRUE)
xpathSApply(html,"//title",xmlValue)

# GET from httr package
if(!require(httr)){
  install.packages("httr",repo="https://cran.rstudio.com/")
}
library(httr)
html2=GET(htmlurl)
content2<-content(html2,as="text")
parsedHtml<-htmlParse(content2,asText=TRUE)
xpathSApply(parsedHtml,"//title",xmlValue)

# accessing websites with password
pg1<-GET("http://httpbin.org/basic-auth/user/passwd") # denied due to authentication block
pg1
pg2<-GET("http://httpbin.org/basic-auth/user/passwd",authenticate("user","passwd")) # authenticated
pg2

# using handles
google<-handle("http://www.google.com")
pg1<-GET(handle=google,path="/") # cookies will be authenticated / maintained
pg2<-GET(handle=google,path="search")

```

# *********************** problem with 5th module *************************** #
###5) reading from APIs
```{r}'''
library(httr)
if(!require(base64enc)){
  install.packages("base64enc",repo="https://cran.rstudio.com/")
}
library(base64enc)
source("hidden.R") # contains consumer key/secret
myapp<-oauth_app("twitter",key=oauth()$consumer_key,secret=oauth()$consumer_secret)
sig<-sign_oauth1.0(app=myapp,token=oauth()$token_key,token_secret = oauth()$token_secret)
homeTL<-GET("https://api.twitter.com/1.1/statuses/home_timeline.json",sig)
json1<-content(homeTL)
json2<-jsonlite::fromJSON(toJSON(json1))
json2[1,1:4]
```'''

# *************************************************************************** #

###6) reading from other sources
```{r}
### files
# file - connection to text file
# url - connection to url
# gzfile - connection to .gz file
# bzfile - connection to .bz2 file

### loads data from Minitab, S, SAS, SPSS, Stata, Systat
# read.arff (Weka)
# read.dta (Stata)
# read.mtp (minitab)
# read.octave (octave)
# read.spss (SPSS)
# read.xport (SAS)

### reading images
# jpeg package
# readbitmap package
# png package
# bioconductor package - EBImage

### reading GIS data
# rgdal package
# rgeos package
# raster package

### reading music data
# tuneR package
# seewave custom package?
```

###7) subsetting / scoring
```{r}
set.seed(13435)
x<-data.frame("var1"=sample(1:5),"var2"=sample(6:10),"var3"=sample(11:15)) 
x<-x[sample(1:5),]; # shuffle around rows 1 through 5
x$var2[c(1,3)]<-NA # mark 1st and 3rd row value of var2 as NA (after shuffling above)
x
x[,1] # subsets column 1
x[,"var3"] # subsets column that matches the name "var3" (3rd col)
x[1:2,"var2"] # subsets row 1,2 of column "var2"
x[(x$var1<=3 & x$var3 > 11),] # select rows that match the var1 value <= 3 and var3>11 (and all cols)
x[(x$var1<=3 | x$var3 > 15),] # select rows that match the var1 value <= 3 OR var3>11 (and all cols)
x[which(x$var2>8),] # subset of x where var2 value is greater than 8 (DEALS WITH MISSING VALUES)
sort(x$var1) # get var1 column and sort it
sort(x$var1,decreasing=TRUE) # same as above but descending
sort(x$var2,na.last=TRUE) # by default (or NA) removes NA, TRUE puts last, FALSE puts first
x[order(x$var1),] # subset of x where it's ordered by var1 value
x[order(x$var1,x$var3),] # same as above, but ordered by var1 and then by var3 (if var1 has ties, var3 is the tiebreaker)

### PLYR ###
library(plyr)
arrange(x,var1) # same as above above, but simpler (and notice lack of "")
arrange(x,desc(var1)) # descending "x[order(x$var1,decreasing=TRUE),]"
#############

x$var4<-rnorm(5) # adds a new column to x as "var4" with rnorm(5) applied
x
x<-cbind(x,"var5"=rnorm(5)) # column bind also works
x
```

###8) summarizing data
```{r}
if(!file.exists("./data")){dir.create("./data")}
if(!file.exists("data/restaurants.csv")){
    fileUrl<-"https://data.baltimorecity.gov/api/views/k5ry-ef3g/rows.csv?accessType=DOWNLOAD"
    download.file(fileUrl,destfile="./data/restaurants.csv",method="curl")
}
restData<-read.csv("./data/restaurants.csv")
head(restData,n=1)
tail(restData,n=1)
summary(restData)
str(restData)
quantile(restData$councilDistrict,na.rm=TRUE)
quantile(restData$councilDistrict,probs=c(0.5,0.75,0.9),na.rm=TRUE) # gets 50%, 75%, 90% quantile
table(restData$zipCode,useNA="ifany") # useNA = "ifany" if there's any use NA, "no" is no, "always" always use it
table(restData$councilDistrict,restData$zipCode) # use councilDistrict as row and column as zipCode
# checking for missing values
sum(is.na(restData$councilDistrict)) # sum of counts of missing values in councilDistrict
any(is.na(restData$councilDistrict)) # is there any missing values in councilDistrict?
all(restData$councilDistrict>0) # all should be greater than 0
all(restData$zipCode>0) # there is at least one negative zip code (invalid) data
colSums(is.na(restData)) # sum of all of the column of restData that has na values
all(colSums(is.na(restData))==0) # all of the column should have 0 value for is.na check

# tally a table of counts when the zipcode matches 21212
table(restData$zipCode %in% c("21212"))
# opposite is not as useful (when 21212 matches the zipcode)
table(c("21212") %in% restData$zipCode)
# also multiple matches work (match 21212 or 21213)
table(restData$zipCode %in% c("21212","21213"))

# subsetting values with specific characteristics
head(restData[restData$zipCode %in% c("21212","21213"),],3)

data(UCBAdmissions)
DF<-as.data.frame(UCBAdmissions)
head(DF)
# cross tabs; Freq is what actually is displayed; Broken down by Gender (row) and Admit (col) 
xt<-xtabs(Freq ~ Gender + Admit,data=DF)
xt
# cross tabs for a larger number of variables are hard to see
warpbreaks$replicate<-rep(1:9,len=54)
head(warpbreaks,3)
# break down breaks by all the variables "." within
xt<-xtabs(breaks~.,data=warpbreaks)
# flat tables;makes it easier to read many variables
# by summarizing/formatting the data in a much smaller and
# compact form
ftable(xt)

# size of a data set
fakeData<-rnorm(1e6)
print(object.size(fakeData),units="Mb")
```

###9) creating new variables
```{r}
### creating new variables along with transformation of data
### variables to do the following:
# 1) missingness indicators
# 2) "cutting up" quantitative variables
# 3) applying transforms

if(!file.exists("./data")){dir.create("./data")}
if(!file.exists("data/restaurants.csv")){
    fileUrl<-"https://data.baltimorecity.gov/api/views/k5ry-ef3g/rows.csv?accessType=DOWNLOAD"
    download.file(fileUrl,destfile="./data/restaurants.csv",method="curl")
}
restData<-read.csv("./data/restaurants.csv")

### review ############################
# creating sequences
# skipping by 2 from 1 to 10 (1,3,5,7,9)
s1<-seq(1,10,by=2); s1 
# from 1 to 10, make it fit the length 3 (1.0, 5.5, 10.0)
s2<-seq(1,10,length=3);s2
# counts along the given list
s3<-seq(along=c(1,3,8,25,100)); s3
# subsetting variables (after creating variable)
restData$nearMe<-restData$neighborhood %in% c("Roland Park", "Homeland")
table(restData$nearMe)
#######################################

# creating binary variables
restData$zipWrong<-ifelse(restData$zipCode<0,TRUE,FALSE)
table(restData$zipWrong,restData$zipCode<0)

# creating categorical variables
restData$zipGroups<-cut(restData$zipCode,breaks<-quantile(restData$zipCode))
table(restData$zipGroups,restData$zipCode)

# easier cutting
if(!require(Hmisc)){
  install.packages("Hmisc",repo="https://cran.rstudio.com/")
}
library(Hmisc)
restData$zipGroups<-cut2(restData$zipCode,g=4)
table(restData$zipGroups)

# creating factor variables
restData$zcf<-factor(restData$zipCode)
restData$zcf[1:10]
class(restData$zcf)

# levels of factor variables
yesno<-sample(c("yes","no"),size=10,replace=TRUE)
# by default, levels value-wise (alphanumeric value)
yesnofac<-factor(yesno,levels=c("yes","no"))
# can manually re-level based on reference value
relevel(yesnofac,ref="yes")

# Hmisc cutting produces factor variables
restData$zipGroups<-cut2(restData$zipCode,g=4)
table(restData$zipGroups)

# using mutate function (Hmisc & plyr) to create new version of the variable 
# and simultaneously added to a data set

if(!require(plyr)){
  install.packages("plyr",repo="https://cran.rstudio.com/")
}
library(plyr)
# mutate old dataframe by adding a new variable "zipGroups" that equals to the function of the original rest data frame
restData2<-mutate(restData,zipGroups=cut2(zipCode,g=4))
table(restData2$zipGroups)
# mutate adds "function" such as ...
#   abs(x)            absolute
#   sqrt(x)           square root
#   ceiling(x)        ceiling
#   floor(x)          floor
#   round(x,digits=n) rounding to digits
#   signif(x,digits=n)rounding to sig fig
#   cos(x),sin(x)     cosine, sine
#   log(x), log2(x)   natural log, log base 2, et cetera
#   exp(x)            exponential of x
```

###10) reshaping data
```{r}
# each variable forms a column
# each observation forms a row
# each table/file stores data regarding one KIND of observation

if(!require(reshape2)){
  install.packages("reshape2",repos = "https://cran.rstudio.com/")
}
library(reshape2)
head(mtcars,n=3)

# melting data frames
mtcars$carname<-rownames(mtcars)
# differentiation of id variables and measure variables
carMelt<-melt(mtcars,id=c("carname","gear","cyl"),measure.vars=c("mpg","hp"))
head(carMelt,n=3) # show mpg variables 
tail(carMelt,n=3) # shows hp variables

# recasting data frames
# left hand is cyl and the variable is set as "mpg" and "hp" previously in melt function above
# without any functions, it aggregates the data set, so 11 in mpg and 11 in hp means 11 measures of mpg and hp
cylData<-dcast(carMelt,cyl~variable);cylData

# by providing the function, it gets / applies the function 'mean' on to each variables
cylData2<-dcast(carMelt,cyl~variable,mean);cylData2

# averaging values
# spray values range from A through F
head(InsectSprays)
# sums the spray counts
tapply(X=InsectSprays$count,INDEX=InsectSprays$spray,FUN=sum)

# another way - split
spIns<-split(x=InsectSprays$count,f=InsectSprays$spray); spIns
# another way - apply
sprCount<-lapply(X=spIns,FUN=sum); sprCount
# another way - combine
unlist(x=sprCount)
sapply(X=spIns,FUN=sum)

# another way - plyr package
if(!require(plyr)){
  install.packages("plyr",repos = "https://cran.rstudio.com/")
}
library(plyr)
# does not work for some reason on knitr; works on R console
#ddply(InsectSprays,.(spray),summarize,sum=sum(count))

# creating a new variable
# same error as above
#spraySums<-ddply(InsectSprays,.(spray),summarize,sum=ave(count,FUN=sum))
#dim(spraySums)
```

###11) managing data frames with dplyr - using the tools
```{r}
if(!require(dplyr)){
  install.packages("dplyr",repos = "https://cran.rstudio.com/")
}
library(dplyr)

options(width=105)

if(!file.exists("./data")){dir.create("./data")}
if(!file.exists("data/chicago.rds")){
    fileUrl<-"https://github.com/DataScienceSpecialization/courses/blob/master/03_GettingData/dplyr/chicago.rds?raw=true"
    download.file(fileUrl,destfile="./data/chicago.rds",mode="wb")
}
chicago<-readRDS("./data/chicago.rds")
# selects from "city" column to "dptp" column
head(select(chicago,city:dptp))

# select everything BUT the city column to dptp column
head(select(chicago,-(city:dptp)))
i<-match("city",names(chicago)); i
j<-match("dptp",names(chicago)); j

# to do the same thing with default R functions...
head(chicago[,-(i:j)])

# filter out pm25tmean2 value that's greater than 30)
chic.f<-filter(chicago,pm25tmean2>30); head(chic.f,3)

# can filter multiple logical statements
chic.f2<-filter(chicago,pm25tmean2>30 & tmpd>80); head(chic.f2,3)

# arrange by date (default: ascending)
chicago<-arrange(chicago, date); head(chicago,3)
chicago<-arrange(chicago, desc(date)); head(chicago,3)

# renaming column (variable); which is annoying to do with vanilla package(s)
# new = old
chicago<-rename(chicago, pm25=pm25tmean2, dewpoint=dptp); head(chicago,3)

# mutate; transform existing variable or create a new variable
chicago<-mutate(chicago, pm25detrend=pm25-mean(pm25,na.rm=TRUE)); head(select(chicago,pm25,pm25detrend))

# group_by; first mutated to include a factor (>80 = hot, <=80 = cold)
chicago<-mutate(chicago, tempcat=factor(1*(tmpd>80),labels=c("cold","hot")))
head(chicago,3);tail(chicago,3)
hotcold<-group_by(chicago,tempcat);hotcold
# average of pm25, maximum value of o3 mean, median of no2 mean
summarize(hotcold,pm25=mean(pm25),o3=max(o3tmean2),no2=median(no2tmean2))
# fix na values for mean
summarize(hotcold,pm25=mean(pm25,na.rm=TRUE),o3=max(o3tmean2),no2=median(no2tmean2))

# ********************************************************************************************* #
# another usage of mutate & group_by;
# for some reason on knitr the following is broken. (dplyr function error); works on console
#chicago<-mutate(chicago,year=as.POSIXlt(date)$year+1900)
#years<-group_by(chicago,year)
# yields the following: 
# Source: local data frame [19 x 4]

#    year     pm25       o3      no2
#   (dbl)    (dbl)    (dbl)    (dbl)
#1   1987      NaN 62.96966 23.49369
#2   1988      NaN 61.67708 24.52296
#3   1989      NaN 59.72727 26.14062
#summarize(years,pm25=mean(pm25,na.rm=TRUE),o3=max(o3tmean2),no2=median(no2tmean2))
# ********************************************************************************************* #

# dplyr pipeline operator:
# take the chicago, pipeline into mutate function, create a new variable "month"
# and then take the output of "mutate", group_by the "month",
# and then take the output of group_by and then run it through summarize
chicago %>% mutate(month=as.POSIXlt(date)$mon + 1) %>% group_by(month) %>% summarize(pm25=mean(pm25,na.rm=TRUE),o3=max(o3tmean2),no2=median(no2tmean2))
# very useful!
```

###12) merging data
```{r}
# downloading necessary files #
if(!file.exists("./data")){dir.create("./data")}
if(!file.exists("./data/reviews.csv")){
  fileUrl1="https://dl.dropboxusercontent.com/u/7710864/data/reviews-apr29.csv"
  download.file(fileUrl1,destfile="./data/reviews.csv",method="curl")
}
if(!file.exists("./data/solutions.csv")){
  fileUrl2="https://dl.dropboxusercontent.com/u/7710864/data/solutions-apr29.csv"
  download.file(fileUrl2,destfile="./data/solutions.csv",method="curl")
}

reviews=read.csv("./data/reviews.csv"); solutions<-read.csv("./data/solutions.csv")

# checking the data are there
head(reviews,3)
head(solutions,3)

# merging data - merge()
# import parameters: x, y, by, by.x, by.y, all
names(reviews); names(solutions)

# merges 'reviews' & 'solutions'
mergedData<-merge(reviews,solutions,by.x="solution_id",by.y="id",all=TRUE)
head(mergedData,3)

# default - merge all common column names
# intersect shows all intersecting column names (default pkg)
intersect(names(solutions),names(reviews))

# merging all common column names 
mergedData2<-merge(reviews,solutions,all=TRUE)
head(mergedData2)

# using "join" in the "plyr" package
df1<-data.frame(id=sample(1:10),x=rnorm(10))
df2<-data.frame(id=sample(1:10),y=rnorm(10))
arrange(join(df1,df2),id)

# if you have multiple data frames; join_all()
# this facilitates a function that is challenging (relatively) with merge()
df3<-data.frame(id=sample(1:10),z=rnorm(10))
dfList<-list(df1,df2,df3);dfList
arrange(join_all(dfList),id)

```

###13) editing text variables
```{r}
# loading necessary data
if(!file.exists("./data")){ dir.create("./data") }
if(!file.exists("./data/cameras.csv")){
  fileUrl<-"https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
  download.file(fileUrl,destfile="./data/cameras.csv",method="curl")
}
cameraData<-read.csv("./data/cameras.csv")

# toupper() & tolower() to fix character vectors
names(cameraData)
toupper(names(cameraData))
tolower(names(cameraData))

# splitting character vectors - strsplit()
# this splits by period "."
# escape characters necessary since "." is a reserved character
splitNames<-strsplit(names(cameraData),"\\.");splitNames

# quick aside - lists
mylist<-list(letters=c("A","b","c"),numbers=1:3,matrix(1:25,ncol=5));mylist
mylist[1]
mylist$letters
mylist[[1]]

# fixing character vectors via sapply()
# to remove the period characters ("Location.1" --> "Location" "1")
splitNames[[6]][1]
splitNames
# this only collects / returns the first element if there are multiple elements
firstElement<-function(x){x[1]}
sapply(splitNames,firstElement)

# downloading necessary files again... and loading
if(!file.exists("./data")){dir.create("./data")}
if(!file.exists("./data/reviews.csv")){
  fileUrl1="https://dl.dropboxusercontent.com/u/7710864/data/reviews-apr29.csv"
  download.file(fileUrl1,destfile="./data/reviews.csv",method="curl")
}
if(!file.exists("./data/solutions.csv")){
  fileUrl2="https://dl.dropboxusercontent.com/u/7710864/data/solutions-apr29.csv"
  download.file(fileUrl2,destfile="./data/solutions.csv",method="curl")
}
reviews<-read.csv("./data/reviews.csv");solutions<-read.csv("./data/solutions.csv")
head(reviews,2);head(solutions,2)

# fixing character vectors - sub()
# substituting out character (replace "_" with "")
names(reviews)
sub("_","",names(reviews))

# fixing character vectors - gsub()
# gsub replaces multiple instances (not just the first ocurrence)
testName<-"this_is_a_test"
sub("_","",testName)
gsub("_","",testName)

# finding values - grep(), grepl()
# look for "Alameda" in cameraData$intersection
# appears in the 4th, 5th, and 36th element of intersection variable
grep("Alameda",cameraData$intersection)

# grepl returns a vector that's true whenever Alameda appears, false otherwise
table(grepl("Alameda",cameraData$intersection))

# subsetting using grepl;
# successfully removes out when intersection variable is "Alameda"
cameraData2<-cameraData[!grepl("Alameda",cameraData$intersection),];head(cameraData2)

# more utilites of grep();
# value=TRUE gets the value instead of the index when GREP matches
grep("Alameda",cameraData$intersection,value=TRUE)

# looking for when the search query does not appear in a variable;
# "JeffStreet" is not found, hence outputs integer 0
grep("JeffStreet",cameraData$intersection)
# and such output is also length zero (which makes it easier to search for)
length(grep("JeffStreet",cameraData$intersection))

# more useful string functions
library(stringr)
nchar("Jeffrey Leek") # number of characters in the input string
substr("Jeffrey Leek",1,7) # substring out the input string from 1st index to 7th
paste("Jeffrey","Leek") # pastes two strings together
paste0("Jeffrey","Leek") # pastes together with no whitespace between
str_trim("     Jeff     ") #trims out whitespaces bidirectionally

# important points about string/texts:
# 1) should be lowercase if possible
# 2) descriptive as opposed to mysterious variables
# 3) non-duplicative
# 4) not have "_", ".", " " if possible
# 5) should be made into factor variables (case by case)
# 6) should be descriptive (use T/F) as opposed to 0/1;
#    and "Male/Female" as opposed to 0/1 and "M/F"
```

###14) working with dates
```{r}
# simple example
d1<-date();d1;class(d1)

# 'Date' class
d2<-Sys.Date();d2;class(d2)

# formatting dates
# %d (days as #), %a (abbreviated weekday), %A (unabbreviated weekday)
# %m (month as #), %b (abbreviated month), %B (unabbreviated month)
# %y (2 digit year), %Y (4 digit year)
format(d2,"%a %b %d")

# creating dates
x<-c("1jan1960","2jan1960","31mar1960","30jul1960")
z<-as.Date(x,"%d%b%Y");z
z[2] - z[1]
as.numeric(z[2]-z[1])

# converting to Julian calendar system
weekdays(d2)
months(d2)
# number of days since the origin of time (1970-01-01)
julian(d2)

# Lubridate package
if(!require(lubridate)){
  install.packages("lubridate",repo="https://cran.rstudio.com/")
}
library(lubridate);
ymd("20140108")
mdy("08/04/2013")
dmy("03-04-2013")

# Lubridate with time + date
ymd_hms("2011-08-03 10:15:03")
ymd_hms("2011-08-03 10:15:03",tz="Pacific/Auckland")

# slightly different syntax for some functions
x<-dmy(c("1jan2013","2jan2013","31mar2013","30jul2013"))
wday(x[1]); wday(x[1],label=TRUE)
```

###15) data resources
```{r}
# open government sites
# United Nations:
#   http://data.un.org
# United States:
#   http:/www.data.gov
# United Kingdom:
#   http://data.gov.uk
# France:
#   http://www.data.gouv.fr
# Ghana:
#   http://data.gov.gh
# Australia:
#   http://data.gov.au
# Germany:
#   https://www.govdata.de
# Hong Kong:
#   http://www.gov.hk/en/theme/psi/datasets
# Japan:
#   http://www.data.go.jp

# Gapminder: data about development of human health
# http://www.gapminder.org

# Survey data from the United States
# http://www.asdfree.com

# Infochimps Marketplace -- some cost money
# http://www.infochimps.com/marketplace

# Kaggle -- data sets
# http://www.kaggle.com

# Collections by data scientists
# Hilary Mason:
#   http://bitly.com/bundles/hmason/1
# Peter Skomoroch
#   https://delicious.com/pksomoroch/datasets
# Jeff Hammerbacher
#   http://www.quora.com/Jeff-Hammerbacher/Introduction-to-Data-Science-Data-Sets
# Gregory Piatetsky-Shapiro
#   http://www.kdnuggets.com/gps.html
# http://blog.mortardata.com/post/67652898761/6-dataset-lists-curated-by-data-scientists

# More specialized collections:
# Stanford Large Network Data
# UCI Machine Learning
# KDD Nuggets Datasets
# CMU Statlib
# Gene Expression Omnibus
# ArXiv Data
# Public Data Sets on Amazon Web Services

# Some API's with R Interfaces
# twitter / twitterR pkg
# figshare / rfigshare
# PLoS / rplos
# rOpenSci
# Facebook / RFacebook
# Google Maps / RGoogleMaps
```