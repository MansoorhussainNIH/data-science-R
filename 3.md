
#Data Science Using R
##3 - Getting and Cleaning Data

###0) R version used

```r
print(R.version.string)
```

```
## [1] "R version 3.2.3 (2015-12-10)"
```

###1) downloading/reading-local/reading-xls/reading-XML/XPath/JSON/data.table
```{r}'''
# downloading
list.files()
if(!file.exists("data")){
    dir.create("data")
}
list.files("./data")

# reading local
if(!file.exists("data/cameras.csv")){
    fileUrl<-"https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
    download.file(fileUrl,destfile="./data/cameras.csv",method="curl")
}
cameraData<-read.table("./data/cameras.csv",sep=",",header=TRUE)
head(cameraData,2)
str(cameraData)

# reading xls
if(!file.exists("data/cameras.xlsx")){
    fileUrl<-"https://data.baltimorecity.gov/api/views/dz54-2aru/rows.xlsx?accessType=DOWNLOAD"
    download.file(fileUrl,destfile="./data/cameras.xlsx",method="curl")
}

if(!require(xlsx)){
  install.packages("xlsx",repo="https://cran.rstudio.com/")
}
library(xlsx)
cameraData<-read.xlsx("./data/cameras.xlsx",sheetIndex=1,header=TRUE)
head(cameraData,2)
str(cameraData)
cameraDataSubset<-read.xlsx("./data/cameras.xlsx",sheetIndex=1,colIndex=2:3,rowIndex=1:4)
head(cameraDataSubset,2)
str(cameraDataSubset)

# reading XML
if(!file.exists("data/simple.xml")){
    fileUrl<-"http://www.w3schools.com/xml/simple.xml"
    download.file(fileUrl,destfile="./data/simple.xml",method="curl")
}
if(!require(XML)){
  install.packages("XML",repo="https://cran.rstudio.com/")
}
library(XML)
doc<-xmlTreeParse("./data/simple.xml",useInternal=TRUE)
rootNode<-xmlRoot(doc)
names(rootNode[[1]])
rootNode[[1]]
rootNode[[1]][[2]]

# programmatically extracting parts of file
xmlSApply(rootNode,xmlValue)
# get the item menu/price using XPath
xpathSApply(rootNode,"//name",xmlValue)
xpathSApply(rootNode,"//price",xmlValue)

# extract content by attributes using XPath
if(!file.exists("data/baltimore-ravens.html")){
    fileUrl<-"http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
    download.file(fileUrl,destfile="./data/baltimore-ravens.html",method="curl")
}
doc<-htmlTreeParse("./data/baltimore-ravens.html",useInternal=TRUE)
xpathSApply(doc,"//li[@class='team-name']",xmlValue)

# reading JSON
if(!file.exists("data/jtleek.json")){
    fileUrl<-"https://api.github.com/users/jtleek/repos"
    download.file(fileUrl,destfile="./data/jtleek.json",method="curl")
}
library(jsonlite)
jsonData<-fromJSON("./data/jtleek.json")
names(jsonData)

#nested objects JSON
library(datasets)
names(jsonData$owner)
testjson<-toJSON(data.frame(foo=1:4,bar=c(T,T,F,F)),pretty=TRUE)
testjson
head(fromJSON(testjson))

# data.table (written in C / VERY FAST)
# inherits from data.frame
if(!require(data.table)){
  install.packages("data.table",repo="https://cran.rstudio.com/")
}
library(data.table)
set.seed(37)
df=data.frame(x=rbinom(5,2,0.5),y=rep(c("a","b","c","d","e")))
dt=data.table(x=rbinom(5,2,0.5),y=rep(c("a","b","c","d","e")))
dt
# subset of data.table
dt[2,] # dt where row is 2 and all of column
dt[dt$y=="a",] # dt where row is where dt's y value equals to "a" and all of col
dt[c(2,3)] # dt where row is 2 and 3 and all of col
dt[,c(2,3)] # subsetting col doesn't work the same as for dataframe

dt[,list(mean(x),sum(x))] # obtains a list of mean of x values of dt, and sum of x
dt[,table(y)] # ~table(dt$y); obtains a table of y values in dt
dt[,z:=x^2] # creates a new col z that is x ^ 2 
dt2<-dt # this does not actually create a new/separate copy
dt2[,y:=2]
head(dt2)
head(dt) # dt2<-dt does not actually create a new copy

# multiple operations within creating a new col
dt[,m:={tmp<-(x+z);log2(tmp+5)}]
head(dt)

# plyr like operations
dt[,a:=x>0]
head(dt)

# by a means where a condition is TRUE, it takes the mean of x and z
# and when a is false, collect those values and perform the func
dt[,b:=sum(x+z),by=a]
head(dt)

# special variables: .N
set.seed(123)
dt<-data.table(x=sample(letters[1:2],5,TRUE))
head(dt)
dt[,.N,by=x] # .N counts by x

# special variables: Keys
dt<-data.table(x=rep(c("a","b","c"),each=3),y=rnorm(3))
setkey(dt,x) # set x as the key so that it can be extracted as below
dt
dt["a"]

dt1<-data.table(x=c("a","a","b","dt1"),y=1:4)
dt2<-data.table(x=c("a","b","dt2"),z=5:7)
setkey(dt1,x); setkey(dt2,x)
dt1
dt2
# hard to interpret, but when x = a, y = 1 and 2 (dt1) and z = 5 (dt2), hence
# x=a;y=1;z=5
# x=a;y=2;z=5
# when x = b, y = 3 (dt1) and z = 6 hence
# x = b;y=3;z=6
# since there is no common x key value for dt1 and dt2 for dt1 and dt2, it is omitted
merge(dt1,dt2) 

# fast reading
big_df<-data.frame(x=rnorm(1E6),y=rnorm(1E6))
file<-tempfile()
write.table(big_df,file=file,row.names=FALSE,col.names=TRUE,sep="\t",quote=FALSE)
if(!require(microbenchmark)){
  install.packages("microbenchmark",repo="https://cran.rstudio.com/")
}
library(microbenchmark)
#fast reading
microbenchmark(fread(file),times=1)
# normal read table
microbenchmark(read.table(file,header=TRUE,sep="\t"),times=1)
```'''

###2) reading MySQL
```{r}'''
if(!require(RMySQL)){
  install.packages("RMySQL",repo="https://cran.rstudio.com/")
}
library(RMySQL)
# connect and obtain result
ucscDb<-dbConnect(MySQL(),user="genome",host="genome-mysql.cse.ucsc.edu")
result<-dbGetQuery(ucscDb,"show databases;"); dbDisconnect(ucscDb);

# connecting to hg19 db and listing all the tables
hg19<-dbConnect(MySQL(),user="genome",db="hg19",host="genome-mysql.cse.ucsc.edu")
allTables<-dbListTables(hg19)
length(allTables)
allTables[1:5]

# get dimensions of specific table
dbListFields(hg19,"affyU133Plus2")
dbGetQuery(hg19,"select count(*) from affyU133Plus2")

# read from a table
affyData<-dbReadTable(hg19,"affyU133Plus2")
str(affyData)

# select a specific subset
query<-dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
affyMis<-fetch(query); quantile(affyMis$misMatches)
affyMisSmall<-fetch(query,n=10); dbClearResult(query);
dim(affyMisSmall)
dbDisconnect(hg19)
```'''

###3) reading HDF5
```{r}'''
# heirarchical data format
# groups contain zero or more data sets/metadata
#       group header with grou name and list of attributes
#       group symbol table with list of objects in group
# datasets multidimensional array of data elements with metadata
#       have header with name,datatype,dataspace,storage layout
#       have data array with data
#  http:/www.hdfgroup.org

if(!file.exists("/data/biocLite.R")){
    fileUrl<-"http://bioconductor.org/biocLite.R"
    download.file(fileUrl,"./data/biocLite.R",method="curl")
}
source("http://bioconductor.org/biocLite.R")
if(!require(rhdf5)){
  biocLite("rhdf5")
}
library(rhdf5)
if(file.exists("example.h5")){
    file.remove("example.h5")
}

created<-h5createFile("example.h5")
created

# create groups
created<-h5createGroup("example.h5","foo")
created<-h5createGroup("example.h5","baa")
created<-h5createGroup("example.h5","foo/foobaa")
h5ls("example.h5")

# write to groups
A<-matrix(1:10,nr=5,nc=2)
h5write(A,"example.h5","foo/A")
B<-array(seq(0.1,2.0,by=0.1),dim=c(5,2,2))
attr(B,"scale")<-"liter"
h5write(B,"example.h5","foo/foobaa/B")
h5ls("example.h5")

# reading data
readA<-h5read("example.h5","foo/A")
readA
readA<-h5read("example.h5","foo/foobaa/B")
readA

# writing and reading chunks
h5write(c(12,13,14),"example.h5","foo/A",index=list(1:3,1)) # write 12,13,14 into rows 1 through 3 column 1 of foo/A within example.h5
h5read("example.h5","foo/A")
```'''

###4) reading from the web
```{r}'''
# just in case google blocks the ip for accessing too often
if(!file.exists("data/google-scholar.html")){
    fileUrl<-"http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
    download.file(fileUrl,"./data/google-scholar.html",method="curl")
}
htmlurl<-"http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
con = url(htmlurl)
htmlCode<-readLines(con)
close(con)

# parsing with XML
library(XML)
html<-htmlTreeParse(htmlurl,useInternalNodes=TRUE)
xpathSApply(html,"//title",xmlValue)

# GET from httr package
library(httr)
html2=GET(htmlurl)
content2<-content(html2,as="text")
parsedHtml<-htmlParse(content2,asText=TRUE)
xpathSApply(parsedHtml,"//title",xmlValue)

# accessing websites with password
pg1<-GET("http://httpbin.org/basic-auth/user/passwd") # denied due to authentication block
pg1
pg2<-GET("http://httpbin.org/basic-auth/user/passwd",authenticate("user","passwd")) # authenticated
pg2

# using handles
google<-handle("http://www.google.com")
pg1<-GET(handle=google,path="/") # cookies will be authenticated / maintained
pg2<-GET(handle=google,path="search")

```'''

###5) reading from APIs
```{r}'''
library(httr)
if(!require(base64enc)){
  install.packages("base64enc",repo="https://cran.rstudio.com/")
}
library(base64enc)
source("hidden.R") # contains consumer key/secret
myapp<-oauth_app("twitter",key=oauth()$consumer_key,secret=oauth()$consumer_secret)
sig<-sign_oauth1.0(app=myapp,token=oauth()$token_key,token_secret = oauth()$token_secret)
homeTL<-GET("https://api.twitter.com/1.1/statuses/home_timeline.json",sig)
json1<-content(homeTL)
json2<-jsonlite::fromJSON(toJSON(json1))
json2[1,1:4]
```'''

###6) reading from other sources
```{r}'''
### files
# file - connection to text file
# url - connection to url
# gzfile - connection to .gz file
# bzfile - connection to .bz2 file

### loads data from Minitab, S, SAS, SPSS, Stata, Systat
# read.arff (Weka)
# read.dta (Stata)
# read.mtp (minitab)
# read.octave (octave)
# read.spss (SPSS)
# read.xport (SAS)

### reading images
# jpeg package
# readbitmap package
# png package
# bioconductor package - EBImage

### reading GIS data
# rgdal package
# rgeos package
# raster package

### reading music data
# tuneR package
# seewave custom package?
```'''

###7) subsetting / scoring
```{r}'''
set.seed(13435)
x<-data.frame("var1"=sample(1:5),"var2"=sample(6:10),"var3"=sample(11:15)) 
x<-x[sample(1:5),]; # shuffle around rows 1 through 5
x$var2[c(1,3)]<-NA # mark 1st and 3rd row value of var2 as NA (after shuffling above)
x
x[,1] # subsets column 1
x[,"var3"] # subsets column that matches the name "var3" (3rd col)
x[1:2,"var2"] # subsets row 1,2 of column "var2"
x[(x$var1<=3 & x$var3 > 11),] # select rows that match the var1 value <= 3 and var3>11 (and all cols)
x[(x$var1<=3 | x$var3 > 15),] # select rows that match the var1 value <= 3 OR var3>11 (and all cols)
x[which(x$var2>8),] # subset of x where var2 value is greater than 8 (DEALS WITH MISSING VALUES)
sort(x$var1) # get var1 column and sort it
sort(x$var1,decreasing=TRUE) # same as above but descending
sort(x$var2,na.last=TRUE) # by default (or NA) removes NA, TRUE puts last, FALSE puts first
x[order(x$var1),] # subset of x where it's ordered by var1 value
x[order(x$var1,x$var3),] # same as above, but ordered by var1 and then by var3 (if var1 has ties, var3 is the tiebreaker)

### PLYR ###
library(plyr)
arrange(x,var1) # same as above above, but simpler (and notice lack of "")
arrange(x,desc(var1)) # descending "x[order(x$var1,decreasing=TRUE),]"
#############

x$var4<-rnorm(5) # adds a new column to x as "var4" with rnorm(5) applied
x
x<-cbind(x,"var5"=rnorm(5)) # column bind also works
x
```'''

###8) summarizing data
```{r}'''
if(!file.exists("./data")){dir.create("./data")}
if(!file.exists("data/restaurants.csv")){
    fileUrl<-"https://data.baltimorecity.gov/api/views/k5ry-ef3g/rows.csv?accessType=DOWNLOAD"
    download.file(fileUrl,destfile="./data/restaurants.csv",method="curl")
}
restData<-read.csv("./data/restaurants.csv")
head(restData,n=1)
tail(restData,n=1)
summary(restData)
str(restData)
quantile(restData$councilDistrict,na.rm=TRUE)
quantile(restData$councilDistrict,probs=c(0.5,0.75,0.9),na.rm=TRUE) # gets 50%, 75%, 90% quantile
table(restData$zipCode,useNA="ifany") # useNA = "ifany" if there's any use NA, "no" is no, "always" always use it
table(restData$councilDistrict,restData$zipCode) # use councilDistrict as row and column as zipCode
# checking for missing values
sum(is.na(restData$councilDistrict)) # sum of counts of missing values in councilDistrict
any(is.na(restData$councilDistrict)) # is there any missing values in councilDistrict?
all(restData$councilDistrict>0) # all should be greater than 0
all(restData$zipCode>0) # there is at least one negative zip code (invalid) data
colSums(is.na(restData)) # sum of all of the column of restData that has na values
all(colSums(is.na(restData))==0) # all of the column should have 0 value for is.na check

# tally a table of counts when the zipcode matches 21212
table(restData$zipCode %in% c("21212"))
# opposite is not as useful (when 21212 matches the zipcode)
table(c("21212") %in% restData$zipCode)
# also multiple matches work (match 21212 or 21213)
table(restData$zipCode %in% c("21212","21213"))

# subsetting values with specific characteristics
head(restData[restData$zipCode %in% c("21212","21213"),],3)

data(UCBAdmissions)
DF<-as.data.frame(UCBAdmissions)
head(DF)
# cross tabs; Freq is what actually is displayed; Broken down by Gender (row) and Admit (col) 
xt<-xtabs(Freq ~ Gender + Admit,data=DF)
xt
# cross tabs for a larger number of variables are hard to see
warpbreaks$replicate<-rep(1:9,len=54)
head(warpbreaks,3)
# break down breaks by all the variables "." within
xt<-xtabs(breaks~.,data=warpbreaks)
# flat tables;makes it easier to read many variables
# by summarizing/formatting the data in a much smaller and
# compact form
ftable(xt)

# size of a data set
fakeData<-rnorm(1e6)
print(object.size(fakeData),units="Mb")
```'''

###9) creating new variables
```{r}'''
### creating new variables along with transformation of data
### variables to do the following:
# 1) missingness indicators
# 2) "cutting up" quantitative variables
# 3) applying transforms

if(!file.exists("./data")){dir.create("./data")}
if(!file.exists("data/restaurants.csv")){
    fileUrl<-"https://data.baltimorecity.gov/api/views/k5ry-ef3g/rows.csv?accessType=DOWNLOAD"
    download.file(fileUrl,destfile="./data/restaurants.csv",method="curl")
}
restData<-read.csv("./data/restaurants.csv")

### review ############################
# creating sequences
# skipping by 2 from 1 to 10 (1,3,5,7,9)
s1<-seq(1,10,by=2); s1 
# from 1 to 10, make it fit the length 3 (1.0, 5.5, 10.0)
s2<-seq(1,10,length=3);s2
# counts along the given list
s3<-seq(along=c(1,3,8,25,100)); s3
# subsetting variables (after creating variable)
restData$nearMe<-restData$neighborhood %in% c("Roland Park", "Homeland")
table(restData$nearMe)
#######################################

# creating binary variables
restData$zipWrong<-ifelse(restData$zipCode<0,TRUE,FALSE)
table(restData$zipWrong,restData$zipCode<0)

# creating categorical variables
restData$zipGroups<-cut(restData$zipCode,breaks<-quantile(restData$zipCode))
table(restData$zipGroups,restData$zipCode)

# easier cutting
if(!require(Hmisc)){
  install.packages("Hmisc",repo="https://cran.rstudio.com/")
}
library(Hmisc)
restData$zipGroups<-cut2(restData$zipCode,g=4)
table(restData$zipGroups)

# creating factor variables
restData$zcf<-factor(restData$zipCode)
restData$zcf[1:10]
class(restData$zcf)

# levels of factor variables
yesno<-sample(c("yes","no"),size=10,replace=TRUE)
# by default, levels value-wise (alphanumeric value)
yesnofac<-factor(yesno,levels=c("yes","no"))
# can manually re-level based on reference value
relevel(yesnofac,ref="yes")

# Hmisc cutting produces factor variables
restData$zipGroups<-cut2(restData$zipCode,g=4)
table(restData$zipGroups)

# using mutate function (Hmisc & plyr) to create new version of the variable 
# and simultaneously added to a data set

if(!require(plyr)){
  install.packages("plyr",repo="https://cran.rstudio.com/")
}
library(plyr)
# mutate old dataframe by adding a new variable "zipGroups" that equals to the function of the original rest data frame
restData2<-mutate(restData,zipGroups=cut2(zipCode,g=4))
table(restData2$zipGroups)
# mutate adds "function" such as ...
#   abs(x)            absolute
#   sqrt(x)           square root
#   ceiling(x)        ceiling
#   floor(x)          floor
#   round(x,digits=n) rounding to digits
#   signif(x,digits=n)rounding to sig fig
#   cos(x),sin(x)     cosine, sine
#   log(x), log2(x)   natural log, log base 2, et cetera
#   exp(x)            exponential of x
```'''

###10) reshaping data
```{r}'''
# each variable forms a column
# each observation forms a row
# each table/file stores data regarding one KIND of observation

if(!require(reshape2)){
  install.packages("reshape2",repos = "https://cran.rstudio.com/")
}
library(reshape2)
head(mtcars,n=3)

# melting data frames
mtcars$carname<-rownames(mtcars)
# differentiation of id variables and measure variables
carMelt<-melt(mtcars,id=c("carname","gear","cyl"),measure.vars=c("mpg","hp"))
head(carMelt,n=3) # show mpg variables 
tail(carMelt,n=3) # shows hp variables

# recasting data frames
# left hand is cyl and the variable is set as "mpg" and "hp" previously in melt function above
# without any functions, it aggregates the data set, so 11 in mpg and 11 in hp means 11 measures of mpg and hp
cylData<-dcast(carMelt,cyl~variable);cylData

# by providing the function, it gets / applies the function 'mean' on to each variables
cylData2<-dcast(carMelt,cyl~variable,mean);cylData2

# averaging values
# spray values range from A through F
head(InsectSprays)
# sums the spray counts
tapply(X=InsectSprays$count,INDEX=InsectSprays$spray,FUN=sum)

# another way - split
spIns<-split(x=InsectSprays$count,f=InsectSprays$spray); spIns
# another way - apply
sprCount<-lapply(X=spIns,FUN=sum); sprCount
# another way - combine
unlist(x=sprCount)
sapply(X=spIns,FUN=sum)

# another way - plyr package
if(!require(plyr)){
  install.packages("plyr",repos = "https://cran.rstudio.com/")
}
library(plyr)
# does not work for some reason on knitr; works on R console
#ddply(InsectSprays,.(spray),summarize,sum=sum(count))

# creating a new variable
# same error as above
#spraySums<-ddply(InsectSprays,.(spray),summarize,sum=ave(count,FUN=sum))
#dim(spraySums)
```'''

###11) managing data frames with dplyr - using the tools

```r
if(!require(dplyr)){
  install.packages("plyr",repos = "https://cran.rstudio.com/")
}
library(dplyr)

options(width=105)

if(!file.exists("./data")){dir.create("./data")}
if(!file.exists("data/chicago.rds")){
    fileUrl<-"https://github.com/DataScienceSpecialization/courses/blob/master/03_GettingData/dplyr/chicago.rds?raw=true"
    download.file(fileUrl,destfile="./data/chicago.rds",mode="wb")
}
chicago<-readRDS("./data/chicago.rds")
# selects from "city" column to "dptp" column
head(select(chicago,city:dptp))
```

```
##   city tmpd   dptp
## 1 chic 31.5 31.500
## 2 chic 33.0 29.875
## 3 chic 33.0 27.375
## 4 chic 29.0 28.625
## 5 chic 32.0 28.875
## 6 chic 40.0 35.125
```

```r
# select everything BUT the city column to dptp column
head(select(chicago,-(city:dptp)))
```

```
##         date pm25tmean2 pm10tmean2 o3tmean2 no2tmean2
## 1 1987-01-01         NA   34.00000 4.250000  19.98810
## 2 1987-01-02         NA         NA 3.304348  23.19099
## 3 1987-01-03         NA   34.16667 3.333333  23.81548
## 4 1987-01-04         NA   47.00000 4.375000  30.43452
## 5 1987-01-05         NA         NA 4.750000  30.33333
## 6 1987-01-06         NA   48.00000 5.833333  25.77233
```

```r
i<-match("city",names(chicago)); i
```

```
## [1] 1
```

```r
j<-match("dptp",names(chicago)); j
```

```
## [1] 3
```

```r
# to do the same thing with default R functions...
head(chicago[,-(i:j)])
```

```
##         date pm25tmean2 pm10tmean2 o3tmean2 no2tmean2
## 1 1987-01-01         NA   34.00000 4.250000  19.98810
## 2 1987-01-02         NA         NA 3.304348  23.19099
## 3 1987-01-03         NA   34.16667 3.333333  23.81548
## 4 1987-01-04         NA   47.00000 4.375000  30.43452
## 5 1987-01-05         NA         NA 4.750000  30.33333
## 6 1987-01-06         NA   48.00000 5.833333  25.77233
```

```r
# filter out pm25tmean2 value that's greater than 30)
chic.f<-filter(chicago,pm25tmean2>30); head(chic.f,3)
```

```
##   city tmpd dptp       date pm25tmean2 pm10tmean2  o3tmean2 no2tmean2
## 1 chic   23 21.9 1998-01-17      38.10   32.46154  3.180556   25.3000
## 2 chic   28 25.8 1998-01-23      33.95   38.69231  1.750000   29.3763
## 3 chic   55 51.3 1998-04-30      39.40   34.00000 10.786232   25.3131
```

```r
# can filter multiple logical statements
chic.f2<-filter(chicago,pm25tmean2>30 & tmpd>80); head(chic.f2,3)
```

```
##   city tmpd dptp       date pm25tmean2 pm10tmean2 o3tmean2 no2tmean2
## 1 chic   81 71.2 1998-08-23       39.6       59.0 45.86364  14.32639
## 2 chic   81 70.4 1998-09-06       31.5       50.5 50.66250  20.31250
## 3 chic   82 72.2 2001-07-20       32.3       58.5 33.00380  33.67500
```
